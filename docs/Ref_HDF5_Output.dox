<!-------------------------------------------->


\page hdf5_output Dakota HDF5 Output

Beginning with release 6.9, %Dakota can write many method 
results such as correlation matrices or the best parameters 
discovered by an optimization to disk in HDF5 format. Many 
users may find accessing their results in HDF5  more convenient 
than scraping or copying and pasting from Dakota's console
output.

To enable HDF5 output, the \ref environment-results_output 
keyword with the \ref environment-results_output-hdf5 option 
must be added to the Dakota input file. In additon, Dakota must 
have been built with HDF5 support. HDF5 support is considered a 
somewhat experimental feature in this release, and therefore binaries
provided on the Download page of the Dakota website do not
have HDF5 support; building from source is necessary. See the
instructions on the Dakota website.

\section hdf5_output-description HDF5 Concepts

HDF5 is a format that is widely used in scientific software for 
efficiently storing and organizing data. The HDF5 standard and 
libraries are maintained by the [HDF Group](https://hdfgroup.org).

In HDF5, data are stored in multidimensional arrays called *datasets*. 
Datasets are organized hierarchically in *groups*, which also can 
contain other groups. Datasets and 
groups are conceptually similar to files and directories in 
a filesystem. In fact, every HDF5 file 
contains at least one group, the root group, denoted "/", and
groups and datasets are accessed using slash delimited absolute or
relative paths.

TODO: Insert an image

A goal of HDF5 is for data to be "self-documenting" through the 
use of metadata. %Dakota output files include two kinds of
metadata.
- Dimension Scales: Each dimension of a dataset may have zero or more
  scales, which are themselves datasets. Scales are often used to 
  provide, for example, labels analogous to column headings 
  in a table (see the dimension scales that %Dakota applies to response
  moments) or numerical values of an indepenent variable (user-specified
  probability levels in level mappings).
- Attributes: key:value pairs that annotate a group or dataset. The keys
  are always string valued, and (in Dakota output) the values can be string,
  integer, or real. %Dakota stores the number of samples that were requested 
  in a \c sampling study in an attribute called 'sampling'.

\section hdf5_output-accessing Accessing Results

Many popular programming languages have support, either natively or from
a third-party library, for reading and writing HDF5 files. The HDF Group
itself supports C/C++ and Java libraries. The Dakota Project suggests 
the \c h5py module for Python. Examples that demonstrate using \c h5py
to access and use Dakota HDF5 output may be found in the Dakota installation
at \c share/dakota/examples/hdf5.

\section hdf5_results Dakota Results

Currently, complete or nearly complete coverage of results from sampling, 
optimization and calibration methods, parameter studies, and stochastic 
expansions exists. Coverage will continue to expand in future releases 
to include not only the results of all methods, but other Dakota output 
such as interface evaluations and model tranformations.

All methods write results to groups and datasets within this path structure:

    /methods/<method_id>/execution:<N>/

The \c /methods group is always present, if a method added results to the output.
The next portion fo the path, `<method_id>`, is the "name" of the method 
block specified by the user with the \ref method-id_method keyword. If the user
provided no name, Dakota uses `NO_ID`. Dakota often automatically generates method 
blocks for its own internal use; the results of these are stored using the name
NOSPEC_ID_*N*, where N is an incrementing integer that begins with 1.

The final portion of the path above, `execution:<N>`, separates the results of
multiple executions of the same method from one another. In simple Dakota studies, 
only a single execution group, `execution:1`, will be present. But in more 
complex studies such as those involving \ref model-nested models, the inner method 
may be executed multiple times, and multiple execution subgroups (`execution:1`, 
`execution:2`, ...) will exist.

The **Expected Output** section of each \ref method's keyword documentation 
indicates the kinds of output, if any, it can write to HDF5. These are 
typically in the form of bulleted lists with clariying notes that refer 
back to the sections that follow. Each type of result has a table that 
describes the datasets where results are stored. The tables include: 

- A brief *description* of the dataset.
- The *location* of the dataset expressed as a path relative to the one just described.
  The path may include both literal text that is always present and replacement text 
  such as \<*response descriptor*\> or \<*variable descriptor*\>, which indicate that 
  the name of a Dakota response or variable makes up a portion of the path.
- The *shape* of the dataset; that is, the number of dimensions
  and the size of each dimension.
- A description of the dataset's *scales*, which includes
  * The *dimension* of the dataset that the scale belongs to.
  * The type (String, Integer, or Real) of the information in the scale.
  * The *label* or name of the scale.
  * The *contents* of the scale. Contents that appear in plaintext are literal and will
    always be present in a scale. Italicized text describes content that varies.
  * *notes* that provide further clarification about the scale.
- A description of the dataset's *attributes*, which are key:value pairs that provide
  helpful context for the dataset.

\subsection hdf5_results-metadata Study Metadata

Several pieces of information about the Dakota study are stored as
attributes of the top-level HDF5 root group ("/"). These include:

\htmlinclude study_attributes_table.html

\subsection hdf5_results-variables A Note about Variables Storage

Variables in most Dakota output (e.g. tabular data files) and input (e.g. imported
data to construct surrogates) are listed in "input spec" order. (Variables are listed
in the \ref variables keyword section by input spec order.) They are first
ordered by function:
1. Design
2. Aleatory
3. Epistemic
4. State

And within each of these categories, they are sorted by domain:
1. Continuous
2. Discrete integer (sets and ranges)
3. Discrete string
4. Discrete real

A shortcoming of HDF5 is that datasets are homogenous; for example, string- and 
real-valued data cannot readily be stored in the same dataset. As a result,
Dakota has chosen to flip "input spec" order when storing variable information
in HDF5 and sort first by domain, then by function. When applicable, there may
be as many as four datasets to store variable information, one to store continuous 
variables, another to store discrete integer variables, and so on. Within these, 
variables will be ordered by function.

\subsection hdf5_results-sampling_moments Sampling Moments

\ref method-sampling produces moments (e.g. mean, standard deviation or
variance) of all responses, as well as 95% lower and upper confidence
intervals for the 1st and 2nd moments. These are stored as described below.
When \ref method-sampling is used in incremental mode by specifying
\ref method-sampling-refinement_samples, all results, including the \c moments group,
are placed within groups named \c increment:<<i>N</i>>, where <i>N</i> indicates
the increment number beginning with 1.

\htmlinclude sampling_moments.html
<br>
\htmlinclude moment_confidence_intervals.html

\subsection hdf5_results-correlations Correlations

A few different methods produce information about the correlations between
pairs of variables and responses (collectively: factors). The four tables
in this section describe how correlation information is stored. One important
note is that HDF5 has no special, native type for symmetric matices, and so
the simple correlations and simple rank correlations are stored in dense 2D
datasets.

\htmlinclude simple_corr.html
<br>
\htmlinclude simple_rank_corr.html
<br>
\htmlinclude partial_corr.html
<br>
\htmlinclude partial_rank_corr.html

\subsection hdf5_results-pdf Probability Density

Some aleatory UQ methods estimate the probability density of resposnes.

\htmlinclude pdf.html

\subsection hdf5_results-level_mappings Level Mappings

Aleatory UQ methods can calculate level mappings (from user-specified probability,
reliability, or generalized reliability to response, or vice versa).

\htmlinclude prob_levels.html
<br>
\htmlinclude rel_levels.html
<br>
\htmlinclude gen_rel_levels.html
<br>
\htmlinclude response_levels.html
 
\subsection hdf5_results-vbd Variance-based Decomposition (Sobol' Indices)
Dakota's \ref method-sampling method can produce main and total effects; stochastic
expansions (\ref method-polynomial_chaos, \ref method-stoch_collocation) additionally
can produce interaction effects.
\htmlinclude vbd_main.html
<br>
\htmlinclude vbd_total.html
Each order (pair, 3-way, 4-way, etc) of interaction is stored in a separate dataset. The
scales are unusual in that they are two-dimensional to contain the labels of the variables
that participate in each interaction.
\htmlinclude vbd_interactions.html


\subsection hdf5_results-se_moments Integration and Expansion Moments
Stochastic expansion methods can obtain moments two ways.
\htmlinclude int_moments.html
<br>
\htmlinclude exp_moments.html

\subsection hdf5_results-extreme_responses Extreme Responses
\ref method-sampling with epistemic variables produces extreme values (minimum and maximum)
for each response.
\htmlinclude extreme_responses.html

\subsection hdf5_results-pstudies Parameter Studies

All parameter studies (\ref method-vector_parameter_study, \ref method-list_parameter_study,
\ref method-multidim_parameter_study, \ref method-centered_parameter_study) record tables of
evaluations (parameter-response pairs), similar to Dakota's tabular output file. Centered 
parameter studies additionally store evaluations in an order that is more natural to intepret, 
which is described below.

In the tabular-like listing, variables are stored according to the scheme described in a 
\ref hdf5_results-variables "previous section".

\htmlinclude parameter_sets.html

Centered paramter studies also store "slices" of the tabular data that make evaluating the
effects of each variable on each response more convenient. The steps for each individual 
variable, including the initial or center point, and corresponding responses are stored 
in separate groups.

\htmlinclude cps_var_slices_steps.html
<br>
\htmlinclude cps_var_slices_resps.html

\subsection hdf5_results-best_params Best Parameters

Dakota's optimization and calibration methods report the parameters at the best point
(or points, for multiple \ref method-final_solutions "final solutions") discovered. These
are stored using the scheme decribed in the \ref hdf5_results-variables "variables" section. When 
more than one solution is reported, the best parameters are nested in groups named `set:<N>`, 
where `N` is a integer numbering the set and beginning with 1.

State (and other inactive variables) are reported when using
\ref responses-objective_functions "objective functions" and for some 
\ref responses-calibration_terms "calibration" studies. However, when using configuration 
variables in a calibration, state variables are suppressed.

\htmlinclude best_parameters.html

\subsection hdf5_results-best_obj_fncs Best Objective Functions

Dakota's optimization methods report the objective functions at the best point (or points,
for multiple \ref method-final_solutions "final solutions") discovered. When 
more than one solution is reported, the best objective functions are nested in groups named `set:<N>`, 
where `N` is a integer numbering the set and beginning with 1.

\htmlinclude best_objective_functions.html

\subsection hdf5_results-best_constraints Best Nonlinear Constraints

Dakota's optimization and calibration methods report the nonlinear constraints at the best 
point (or points, for multiple \ref method-final_solutions "final solutions") discovered. When 
more than one solution is reported, the best constraints are nested in groups named `set:<N>`, 
where `N` is a integer numbering the set and beginning with 1.


\htmlinclude best_constraints.html

\subsection hdf5_results-residuals Calibration

When using \ref responses-calibration_terms "calibration terms" with an optimization method, or 
when using a nonlinear least squares method such as \ref method-nl2sol, Dakota reports residuals
and residual norms for the best point (or points, for multiple 
\ref method-final_solutions "final solutions") discovered.

\htmlinclude best_residuals.html
<br>
\htmlinclude best_residual_norm.html

\subsection hdf5_results-best_model_responses Best Model Responses

When performing calibration with experimental data (but with no configruation variables),
Dakota records, in addition to the best residuals, the best original model resposnes.

\htmlinclude best_model_responses.html

\subsection hdf5_results-best_experiment Best Model Responses (with configuration variables)

When performing calibration with experimental data that includes configuration variables, Dakota
reports the best model responses for each experiment. These results include the configuration 
variables, stored in the scheme described in the \ref hdf5_results-variables "variables" section,
and the model responses.

\htmlinclude best_config_vars.html
<br>
\htmlinclude best_model_resps_config_vars.html

